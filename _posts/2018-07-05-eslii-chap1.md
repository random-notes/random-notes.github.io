---
layout: post
title:  "ESLII Chapter 1 Notes"
date:   2018-07-05 18:52:21
categories: eslii
---
# Introduction

## Conventions
* Inputs: inputs, predictors, independent variables
* Outputs: outputs, responses, dependent variables
* Prediction Tasks: 
	* Regression: predicting quantitative outputs
	* Classification: predicting qualitative outputs
* Notations:
	* General aspect of a variable:
		* $X$ input, $Y$ quantitative output, $G$ qualitative output
	* Observed values
		* $x_i$ the $i^{\textsf{th}}$ observed input in $X$
		* $\mathbf{X}\in\mathbb{R}^{N\times p}$, matrix $\mathbf{X}$ contains $N$ inputs, each is a $p$-vector $x_i$. $i^{\textsf{th}}$ row in $\mathbf{X}$ denotes as $\mathbf{X}_i = x_i^T$, which is the transpose of $i^{\textsf{th}}$ input $x_i$ in $X$.
		* $\mathbf{x}_j$ denotes the $j^{\textsf{th}}$ feature in $\mathbf{X}$.
* All vectors are **column vectors**, that is $v \in \mathbb{R}^{m \times 1}$.

## Linear Models and Least Squares
Given an input vector with $p$-features $X^T = (X_1, X_2,\ldots,X_p)$, a linear model predicts the output $Y$ using 

$$\hat{Y}=\hat{\beta}_0 + \sum_{j=1}^{p}X_j\hat{\beta}_j,$$

in which $\hat{\beta}_0$ is the intercept (bias), and $\hat{\beta}_j$ are coefficients. By adding an extra $1$ into $X^T$, we can get $X^T = (1, X_1, X_2, \ldots, X_p)$ and therefore rewrite the equation to

$$\hat{Y} = X^T\hat{\beta},$$

in which $X \in \mathbb{R}^{(p + 1) \times 1}$ and $\hat{\beta} \in \mathbb{R}^{(p + 1) \times 1}$.

If $Y$ is not a scalar but a $K$-dimensional vector, then $\beta \in \mathbb{R}^{(p+1)\times K}$.

The gradient of function $f(X)=X^T\beta$ is $f^\prime(X)=\beta$. To get a better idea on how to calculate gradients, please refer to [Derivative], [Partial Derivative] and [Chain Rule] from MathWorld.

> TODO: Add a note for derivatives.

### Least Squares

To fit the linear model $f(X)$ to $N$-training instances $(x_i, y_i)$, we use the coefficients $\beta$ to minimize the residual sum of squares ([RSS])

$$RSS(\beta) = \sum_{i=1}^{N}(y_i - x_i^T\beta)^2.$$

Because $RSS(\beta)$ is a [quadratic function] that always $\geq 0$, it always have a minimum value, but the corresponding $\beta$ might not be unique. 

> TODO: Put an example here.

The solution of $RSS(\beta)$ can be characterize in matrix notation 

$$RSS(\beta) = (\mathbf{y} - \mathbf{X}\beta)^T(\mathbf{y} - \mathbf{X}\beta),$$

in which $\mathbf{X}\in\mathbb{R}^{N\times p}$, $\mathbf{y}\in\mathbb{R}^{N\times 1}$. Differentitating w.r.t $\beta$ will results in the [normal equation]

$$\mathbf{X}^{T}(\mathbf{y} - \mathbf{X}\beta)=0,$$

and if $\mathbf{X}^T\mathbf{X}$ is [nonsingular], then the unique solution is given by

$$\hat{\beta}=(\mathbf(X)^T\mathbf{X})^{-1}\mathbf{T}\mathbf{y},$$

the explanation on [Residual sum of squares -- Wiki] is much easier to understand from my opinion. Basically, we know that 

$$\mathbf{X}\hat\beta = \mathbf{y}$$

then

$$\mathbf{X}^T\mathbf{X}\hat\beta = \mathbf{X}^T\mathbf{y}$$

$$\hat\beta = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$


To learn more about how to calculate quadratic derivatives, please refer to [Michael Orlitzky's post] and this [residual sum of squares in closed form].

[Derivative]: http://mathworld.wolfram.com/Derivative.html
[Partial Derivative]: http://mathworld.wolfram.com/PartialDerivative.html
[Chain Rule]: http://mathworld.wolfram.com/ChainRule.html
[RSS]: https://en.wikipedia.org/wiki/Residual_sum_of_squares
[quadratic function]: http://dl.uncw.edu/digilib/Mathematics/Algebra/mat111hb/PandR/quadratic/quadratic.html
[normal equation]: http://mathworld.wolfram.com/NormalEquation.html
[Michael Orlitzky's post]: http://michael.orlitzky.com/articles/the_derivative_of_a_quadratic_form.xhtml
[residual sum of squares in closed form]: https://math.stackexchange.com/questions/756679/least-squares-residual-sum-of-squares-in-closed-form
[nonsingular]: http://mathworld.wolfram.com/NonsingularMatrix.html
[Residual sum of squares -- Wiki]: https://en.wikipedia.org/wiki/Residual_sum_of_squares

```javascript
<!-- This is used to add an iframe -->
<div class='resp-container'>
<iframe class='resp-iframe' height='265' scrolling='no' title='bKJLqW' src='https://codepen.io/bxshi/embed/bKJLqW/?height=265&theme-id=0&default-tab=result&embed-version=2' frameborder='no' allowtransparency='  ' allowfullscreen='  '>
</iframe>
</div>
```

<!-- This is used to add an iframe -->
<div class='resp-container'>
<iframe class='resp-iframe' height='265' scrolling='no' title='bKJLqW' src='https://codepen.io/bxshi/embed/bKJLqW/?height=265&theme-id=0&default-tab=result&embed-version=2' frameborder='no' allowtransparency='  ' allowfullscreen='  '>
</iframe>
</div>
